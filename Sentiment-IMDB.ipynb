{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h1 align=\"center\">Lesson 5: Text Classification - Sentiment Analysis</h1>\n",
    "    <h3 align=\"center\">Javad Mohammadzadeh\n",
    "    </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Text Classification:</h6> Assigning a label (from a set of pre-defined labels) to a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Some applications:</h6>\n",
    "- **Sentiment Analysis:** Determining the polarity of a text (`positive` or `negative`).\n",
    "- **Spam detection:** email, web pages, etc.\n",
    "- **Toxic text detection:** insult, racism, hatred, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# sys.path.append(r'C:\\Users\\Student\\Desktop\\prg')\n",
    "# print(sys.path)\n",
    "\n",
    "from utils import *\n",
    "from data_utils import Vocabulary, tokenizer\n",
    "from train_utils import train\n",
    "\n",
    "# setup\n",
    "use_gpu = torch.cuda.is_available()\n",
    "NLP = spacy.load('en_core_web_sm')  # NLP toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Bromwell High is a cartoon comedy. \n",
    "It ran at the same time as some other programs about school life, such as 'Teachers'. \n",
    "My 35 years in the teaching profession lead me to believe that Bromwell High's \n",
    "satire is much closer to reality than is 'Teachers'. \n",
    "The scramble to survive financially, the insightful students who can see \n",
    "right through their pathetic teachers' pomp, the pettiness of the whole situation, \n",
    "all remind me of the schools I knew and their students. \n",
    "When I saw the episode in which a student repeatedly tried to burn down the school, \n",
    "I immediately recalled ......... at .......... High. \n",
    "A classic line: INSPECTOR: I'm here to sack one of your teachers. \n",
    "STUDENT: Welcome to Bromwell High. \n",
    "I expect that many adults of my age think that Bromwell High is far fetched. \n",
    "What a pity that it isn't!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy.  It ran at the same time as some other programs about school life, such as 'Teachers'.  My 35 years in the teaching profession lead me to believe that Bromwell High's  satire is much closer to reality than is 'Teachers'.  The scramble to survive financially, the insightful students who can see  right through their pathetic teachers' pomp, the pettiness of the whole situation,  all remind me of the schools I knew and their students.  When I saw the episode in which a student repeatedly tried to burn down the school,  I immediately recalled ......... at .......... High.  A classic line  INSPECTOR  I'm here to sack one of your teachers.  STUDENT  Welcome to Bromwell High.  I expect that many adults of my age think that Bromwell High is far fetched.  What a pity that it isn't!!! \n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as 'Teachers'. My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is 'Teachers'. The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line INSPECTOR I'm here to sack one of your teachers. STUDENT Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!!! \n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r\"[ ]+\", \" \", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as 'Teachers'. My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is 'Teachers'. The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line INSPECTOR I'm here to sack one of your teachers. STUDENT Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't! \n"
     ]
    }
   ],
   "source": [
    "text = re.sub(r\"\\!+\", \"!\", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', \"'\", 'Teachers', \"'\", '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', \"'\", 'Teachers', \"'\", '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', 'INSPECTOR', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = [w.text for w in NLP.tokenizer(text)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a function in `utils.py`, which gets the inputs text and split it to a sequence of tokens. We have used **SpaCy** toolkit for tokeniztion and you need to install it to run the codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "def tokenizer(text):\n",
    "    text = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;]\", \" \", str(text))\n",
    "    text = re.sub(r\"[ ]+\", \" \", text)\n",
    "    text = re.sub(r\"\\!+\", \"!\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = re.sub(r\"\\?+\", \"?\", text)\n",
    "    return [x.text for x in NLP.tokenizer(text) if x.text != \" \"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install SpaCy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation:\n",
    "<pre>conda install -c conda-forge spacy</pre>\n",
    "\n",
    "Download a language:\n",
    "<pre>python -m spacy download en</pre>\n",
    "\n",
    "Usage:\n",
    "```python\n",
    "import spacy\n",
    "NLP = spacy.load('en')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[IMDB](http://ai.stanford.edu/~amaas/data/sentiment/) Dataset\n",
    "- A dataset for binary sentiment classification.\n",
    "- It provides a set of 25,000 highly polar movie reviews for training, and 25,000 for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:\\Users\\Student\\Desktop\\prg\\aclImdb'\n",
    "\n",
    "vocab_path = 'vocab.pkl'\n",
    "\n",
    "# parameters\n",
    "max_len = 200\n",
    "min_count = 10\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(f'{data_dir}/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-8081751c11bd>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  num_words = [len(open(f).read().split(' ')) for f in tqdm_notebook(all_filenames)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85789f0f7874b45ba7342c90205501d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length = 4\n",
      "Max length = 2470\n",
      "Mean = 231.15\n",
      "Std  = 171.32\n",
      "mean + 2 * sigma = 573.80\n"
     ]
    }
   ],
   "source": [
    "all_filenames = glob(f'{data_dir}/*/*/*.txt')\n",
    "num_words = [len(open(f).read().split(' ')) for f in tqdm_notebook(all_filenames)]\n",
    "\n",
    "# print statistics\n",
    "print('Min length =', min(num_words))\n",
    "print('Max length =', max(num_words))\n",
    "\n",
    "print('Mean = {:.2f}'.format(np.mean(num_words)))\n",
    "print('Std  = {:.2f}'.format(np.std(num_words)))\n",
    "\n",
    "print('mean + 2 * sigma = {:.2f}'.format(np.mean(num_words) + 2.0 * np.std(num_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168, 234, 225, 308, 242, 116, 164, 140, 129, 161, 113, 339, 247, 112, 185, 213, 148, 196, 350, 78]\n"
     ]
    }
   ],
   "source": [
    "print(num_words[:20])\n",
    "PAD = '<pad>'  # special symbol we use for padding text\n",
    "UNK = '<unk>'  # special symbol we use for rare or unknown word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, tokenizer, \n",
    "                 split='train', \n",
    "                 vocab_path='vocab.pkl', \n",
    "                 max_len=100, min_count=10):\n",
    "        \n",
    "        self.path = path\n",
    "        assert split in ['train', 'test']\n",
    "        self.split = split\n",
    "        self.vocab_path = vocab_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.min_count = min_count\n",
    "        \n",
    "        self.cache = {}\n",
    "        self.vocab = None\n",
    "        \n",
    "        self.classes = []\n",
    "        self.class_to_index = {}\n",
    "        self.text_files = []\n",
    "        \n",
    "        split_path = f'{path}/{split}'\n",
    "        for cls_idx, label in enumerate(os.listdir(split_path)):\n",
    "            text_files = [(fname, cls_idx) for fname in glob(f'{split_path}/{label}/*.txt')]\n",
    "            self.text_files += text_files\n",
    "            self.classes += [label]\n",
    "            self.class_to_index[label] = cls_idx\n",
    "        \n",
    "        self.num_classes = len(self.classes)\n",
    "            \n",
    "        # build vocabulary from training and validation texts\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def __getitem__(self, index):#[]\n",
    "        # read the tokenized text file and its label (neg=0, pos=1)\n",
    "        fname, class_idx = self.text_files[index]\n",
    "        \n",
    "        if fname in self.cache:\n",
    "            return self.cache[fname], class_idx\n",
    "        \n",
    "        # read text file \n",
    "        text = open(fname).read()\n",
    "        \n",
    "        # tokenize the text file\n",
    "        tokens = self.tokenizer(text.lower())\n",
    "        \n",
    "        # padding and trimming\n",
    "        if len(tokens) < self.max_len:\n",
    "            num_pads = self.max_len - len(tokens)\n",
    "            tokens = [PAD] * num_pads + tokens\n",
    "        elif len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "            \n",
    "        # numericalizing\n",
    "        ids = torch.LongTensor(self.max_len)\n",
    "        for i, word in enumerate(tokens):\n",
    "            if word not in self.vocab.word2index:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # unknown words\n",
    "            elif word != PAD and self.vocab.word2count[word] < self.min_count:\n",
    "                ids[i] = self.vocab.word2index[UNK]  # rare words\n",
    "            else:\n",
    "                ids[i] = self.vocab.word2index[word]\n",
    "                \n",
    "        # save in cache for future use\n",
    "        self.cache[fname] = ids\n",
    "        \n",
    "        return ids, class_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_files)\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        if not os.path.exists(self.vocab_path):\n",
    "            vocab = Vocabulary(self.tokenizer)\n",
    "            filenames = glob(f'{path}/*/*/*.txt')\n",
    "            for filename in tqdm(filenames, desc='Building Vocab'):\n",
    "                with open(filename, encoding='utf8') as f:\n",
    "                    for line in f:\n",
    "                        vocab.add_sentence(line.lower())\n",
    "\n",
    "            # sort words by their frequencies\n",
    "            words = [(0, PAD), (0, UNK)]\n",
    "            words += sorted([(c, w) for w, c in vocab.word2count.items()], reverse=True)\n",
    "\n",
    "            self.vocab = Vocabulary(self.tokenizer)\n",
    "            for i, (count, word) in enumerate(words):\n",
    "                self.vocab.word2index[word] = i\n",
    "                self.vocab.word2count[word] = count\n",
    "                self.vocab.index2word[i] = word\n",
    "                self.vocab.count += 1\n",
    "\n",
    "            pickle.dump(self.vocab, open(self.vocab_path, 'wb'))\n",
    "        else:\n",
    "            self.vocab = pickle.load(open(self.vocab_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = TextClassificationDataset(data_dir, tokenizer, 'train', vocab_path, max_len, min_count)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_ds = TextClassificationDataset(data_dir, tokenizer, 'test', vocab_path, max_len, min_count)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0, 'pos': 1}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0   761   299\n",
      "     3    13   130    14     9   443     8    34    40   504    25     3\n",
      "    24  1372     3    38   143    33  2713    54  1963    36     2  2703\n",
      "    42    93    87  2297    75  3924    54     5    29   115     4   265\n",
      "     2   138  1719     5 13564   473    19   326     3    14    76     9\n",
      "   108  1381     8   115     4     2  4078     7     6   452  1353    19\n",
      "  1783     4     2     1   645    78   108   204     4    98   204    70\n",
      "    26   115   117    93    47  1233    45     5   670    59   266    60\n",
      "     2   421     9   117    93  5045     4    13    73    77     6   256\n",
      "    69    47   388   151    14    25     4     2   380    78   326     3\n",
      "    24    98    86     7    15   458     3 15115     3  3367     3   535\n",
      "    70    26   198    59     2    72   158    13   432    19  9273 16644\n",
      "     5    50   815  2654     5  1093   138     4   908    14    19     6\n",
      "  7603     7  3085     4     5    13   155    66   332  1567    32   371\n",
      "     4    13   112   252    41   550    38   513   342   342   246   635\n",
      "     7    14     9     3    82     3  3175     4]\n",
      "neg\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0   761   299\n",
      "     3    13   130    14     9   443     8    34    40   504    25     3\n",
      "    24  1372     3    38   143    33  2713    54  1963    36     2  2703\n",
      "    42    93    87  2297    75  3924    54     5    29   115     4   265\n",
      "     2   138  1719     5 13564   473    19   326     3    14    76     9\n",
      "   108  1381     8   115     4     2  4078     7     6   452  1353    19\n",
      "  1783     4     2     1   645    78   108   204     4    98   204    70\n",
      "    26   115   117    93    47  1233    45     5   670    59   266    60\n",
      "     2   421     9   117    93  5045     4    13    73    77     6   256\n",
      "    69    47   388   151    14    25     4     2   380    78   326     3\n",
      "    24    98    86     7    15   458     3 15115     3  3367     3   535\n",
      "    70    26   198    59     2    72   158    13   432    19  9273 16644\n",
      "     5    50   815  2654     5  1093   138     4   908    14    19     6\n",
      "  7603     7  3085     4     5    13   155    66   332  1567    32   371\n",
      "     4    13   112   252    41   550    38   513   342   342   246   635\n",
      "     7    14     9     3    82     3  3175     4]\n"
     ]
    }
   ],
   "source": [
    "ids, label = train_ds[3]\n",
    "print(len(ids.numpy()))\n",
    "print(ids.numpy())\n",
    "\n",
    "print(train_ds.classes[label])\n",
    "print(ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> sorry everyone , i know this is supposed to be an art film , but wow , they should have handed out guns at the screening so people could blow their brains out and not watch . although the scene design and photographic direction was excellent , this story is too painful to watch . the absence of a sound track was brutal . the <unk> shots were too long . how long can you watch two people just sitting there and talking ? especially when the dialogue is two people complaining . i really had a hard time just getting through this film . the performances were excellent , but how much of that dark , sombre , uninspired , stuff can you take ? the only thing i liked was maureen stapleton and her red dress and dancing scene . otherwise this was a ripoff of bergman . and i 'm no fan f his either . i think anyone who says they enjoyed 1 1 2 hours of this is , well , lying .\n"
     ]
    }
   ],
   "source": [
    "# convert back the sequence of integers into original text\n",
    "ids=ids.numpy()\n",
    "print(' '.join([train_ds.vocab.index2word[i] for i in ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "# print the original text\n",
    "print(open(train_ds.text_files[0][0]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vovcabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 29512\n",
      "\n",
      "Most common words:\n",
      "the: 666690\n",
      ",: 543308\n",
      ".: 469782\n",
      "and: 324154\n",
      "a: 321797\n",
      "of: 289312\n",
      "to: 267959\n",
      "is: 217022\n",
      ">: 202239\n",
      "it: 187957\n",
      "in: 186452\n",
      "i: 173859\n",
      "this: 150166\n",
      "that: 143987\n",
      "'s: 122009\n",
      "<: 119154\n",
      "br: 119019\n",
      "was: 99882\n",
      "as: 91665\n",
      "for: 87301\n",
      "with: 87183\n",
      "movie: 86122\n",
      "but: 83310\n",
      "film: 78079\n",
      "you: 68703\n",
      "on: 67730\n",
      "n't: 66056\n",
      "not: 62603\n",
      "are: 60025\n",
      "he: 58686\n",
      "his: 57501\n",
      "have: 56757\n",
      "be: 53208\n",
      "one: 53082\n",
      "at: 46724\n",
      "all: 46418\n",
      "they: 45218\n",
      "by: 44432\n",
      "an: 42945\n",
      "who: 42196\n",
      "so: 40701\n",
      "from: 40439\n",
      "like: 40089\n",
      "there: 37011\n",
      "or: 35736\n",
      "just: 35142\n",
      "do: 34982\n",
      "!: 34878\n",
      "her: 34548\n",
      "about: 33987\n"
     ]
    }
   ],
   "source": [
    "vocab = train_ds.vocab\n",
    "freqs = [(count, word) for (word, count) in vocab.word2count.items() if count >= min_count]\n",
    "vocab_size = len(freqs) + 2  # for PAD and UNK tokens\n",
    "print(f'Vocab size = {vocab_size}')\n",
    "\n",
    "print('\\nMost common words:')\n",
    "for c, w in sorted(freqs, reverse=True)[:50]:\n",
    "    print(f'{w}: {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, num_classes, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) # a lookup table\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, dropout=0.3, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*hidden_size, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(100, num_classes)\n",
    "        )\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        h = to_var(torch.zeros((2*self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        c = to_var(torch.zeros((2*self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, self.hidden = self.lstm(x, self.hidden)\n",
    "        x = self.fc(x[-1])  # select the last output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29512\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 2 + len([w for (w, c) in train_ds.vocab.word2count.items() if c >= min_count])\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM parameters\n",
    "embed_size = 100\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "\n",
    "# training parameters\n",
    "lr = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(embed_size=embed_size, \n",
    "                       hidden_size=hidden_size, \n",
    "                       vocab_size=vocab_size,\n",
    "                       num_layers=num_layers,\n",
    "                       num_classes=train_ds.num_classes, \n",
    "                       batch_size=batch_size)\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "if use_gpu:\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.7, 0.99))\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('models/lstm-3-400-10-256-1024-1-0.85764.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08198afa96d46f19388776d3706de5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Student\\Desktop\\prg\\train_utils.py:32: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 0.3)\n"
     ]
    }
   ],
   "source": [
    "hist = train(model, train_dl, valid_dl, criterion, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSTM parameters\n",
    "max_len = 400\n",
    "min_count = 10\n",
    "embed_size = 256\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "\n",
    "\n",
    "model = LSTMClassifier(embed_size=embed_size, \n",
    "                       hidden_size=hidden_size, \n",
    "                       vocab_size=vocab_size,\n",
    "                       num_layers=num_layers,\n",
    "                       num_classes=train_ds.num_classes, \n",
    "                       batch_size=batch_size)\n",
    "\n",
    "model.load_state_dict(torch.load('models/tmp/lstm-1-400-10-256-1024-1-0.87964.pth'))\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/wordvecs_persian.png' width='80%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vecs = model.embedding.weight.data =  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([29512, 100])\n"
     ]
    }
   ],
   "source": [
    "print(word_vecs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.0678\n",
      "-0.8920\n",
      "-0.0058\n",
      "-1.3950\n",
      " 0.8038\n",
      " 0.9048\n",
      " 0.1682\n",
      "-0.2589\n",
      " 2.0809\n",
      "-1.5028\n",
      "-0.1373\n",
      " 0.9797\n",
      "-1.3226\n",
      " 0.1690\n",
      " 0.1304\n",
      " 1.2229\n",
      "-1.5732\n",
      "-0.0680\n",
      " 0.2411\n",
      "-0.5359\n",
      "-1.1798\n",
      " 1.5737\n",
      "-0.0653\n",
      "-1.5709\n",
      "-0.1531\n",
      "-1.6529\n",
      " 0.9688\n",
      " 0.0960\n",
      "-0.0773\n",
      "-0.0464\n",
      "-1.1947\n",
      "-0.5481\n",
      " 0.2004\n",
      "-2.5060\n",
      " 0.3788\n",
      "-1.3304\n",
      "-0.2268\n",
      " 0.5360\n",
      "-0.7743\n",
      "-1.7638\n",
      "-0.7493\n",
      "-0.5174\n",
      "-0.4552\n",
      " 0.3821\n",
      "-0.7965\n",
      " 0.0837\n",
      "-0.1656\n",
      "-0.3691\n",
      " 1.0137\n",
      "-0.6089\n",
      " 0.8563\n",
      "-0.4448\n",
      "-0.5062\n",
      " 0.9916\n",
      " 1.4466\n",
      "-0.2972\n",
      " 0.3063\n",
      " 0.3336\n",
      " 0.1705\n",
      "-1.5537\n",
      "-0.1382\n",
      "-0.5471\n",
      "-1.1656\n",
      " 0.6987\n",
      " 0.7385\n",
      " 1.0847\n",
      "-1.3165\n",
      " 1.1401\n",
      " 0.0106\n",
      " 0.5664\n",
      "-0.6338\n",
      " 0.0254\n",
      "-0.0188\n",
      "-0.9588\n",
      "-0.4423\n",
      " 0.2074\n",
      " 0.3086\n",
      "-0.6075\n",
      "-1.9901\n",
      " 0.5488\n",
      "-1.5887\n",
      "-1.1958\n",
      " 0.6825\n",
      "-0.4171\n",
      " 0.7192\n",
      "-0.7814\n",
      "-1.2733\n",
      " 0.7871\n",
      "-0.2044\n",
      " 0.6582\n",
      " 1.0289\n",
      " 0.9307\n",
      "-0.0660\n",
      " 0.9300\n",
      " 0.0040\n",
      " 0.5446\n",
      " 1.5127\n",
      " 0.8484\n",
      "-0.9488\n",
      " 0.6794\n",
      "[torch.cuda.FloatTensor of size 100 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(word_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fine-tuning hyper-parameters\n",
    "- Bidirectional LSTM\n",
    "- Pre-trained word vectors ([GloVe](https://nlp.stanford.edu/projects/glove/), [FastText](https://code.facebook.com/posts/1438652669495149/fair-open-sources-fasttext/), etc.)\n",
    "- Dropouts and regularization (https://arxiv.org/pdf/1708.02182.pdf)\n",
    "- Attention mechanism (https://distill.pub/2016/augmented-rnns/)\n",
    "- Use GRU or QRNN (https://arxiv.org/pdf/1611.01576.pdf)\n",
    "- Pre-training with a language model (https://arxiv.org/pdf/1605.07725.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/attention-example.png' width='90%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/QRNN.png' width='100%'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Excersize: Toxic Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try to get an accuracy equal (or above 90) % for IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>LSTM</h6> \n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Word Vectors and NLP</h6>\n",
    "- https://einstein.ai/research/learned-in-translation-contextualized-word-vectors"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Untitled.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
